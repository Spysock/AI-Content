# -*- coding: utf-8 -*-
"""00_pytorch_fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wvRdCgTBESWOoi8z_srzkVghW0nleT_w
"""

print("Hello I'm excited to learn PyTorch")

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

#scalars
scalar = torch.tensor(7)
scalar

scalar.ndim

#Get tensor back as Python Int (Exclusive to scalar)
scalar.item()

#Vector
vector = torch.tensor([7,7])
vector

vector.ndim

#Get tensor size back, (Good to find size of vector)
vector.shape

#.shape works for scalars as well
scalar.shape

#Matrix (Has properties of a matrix so you can't have [[1,2,3],[1,2]])
#Creating 3x2 is possible
MATRIX = torch.tensor([[7,8],[9,10]])
MATRIX

MATRIX.ndim

#First number is the amount of []
#Second number is the amount of items within each []
MATRIX.shape

#TENSOR
TENSOR = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]]])
TENSOR

#Every square bracket surrounding a large group is a new dimension
TENSOR.ndim

#First Number is the amount of brackets encapsulating the large amount of stuff
#Second number is the amount of brackets within the first bracket
#Third number is the items within the brackets of the second
TENSOR.shape

TENSOR[0][0][0]

#float 32 Tensor
#these data types are the 3 most important ones when creating a tensor
#Tensor datatypes is one of 3 big errors you'll run into with PyTorch and deeplearning
#1. Tensor not right data type
#2. Tensors not right shape
#3. Tensors not on right device

float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype = None, #What data type is the tensor
                               device = 'cpu', #default it's cpu, can also change to cuda, what device your tenosr is on
                               requires_grad = False,) #Can track gradients for you, whether or not to track gradients for you
float_32_tensor

#if dtype specified as None it'll come out as 32
float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

#They can multiply
#So can floats and ints
#And different bit types
float_16_tensor * float_32_tensor

### Getting information from our tensors
# - To get datatype from a tensor can use , 'tensor.dtype'
# - To get shape from tensor you can use, tensor.shape
# - To get device from tensor use, tensor.device

some_tensor = torch.rand(3,4)
some_tensor

print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Device of tensor: {some_tensor.device}")

###Manipulating Tensors (tensor operations)

#Tensor Operations
# - Addition
# - Subtraction
# - Multiplication
# - Division
# - Matrix Multiplication

#Addition
tensor = torch.tensor([1,2,3])
tensor + 10

#Multiplication
tensor * 10

#Must be reassigned in order to save the values

#tensor = tensor * 10
#tensor
#Out put will be tensor([10,20,30])

#Subtraction
tensor -10

#PyTorch in-built functions
#Multiplication
torch.mul(tensor, 10)

#Addition
torch.add(tensor,10)

###Matrix Multiplication

#2 Main ways preform multiplication in NN's and Deep learning:
#1. Element-wise
#2. Matrix Multiplication, Most common way

#Matrix Multiplication
torch.matmul(tensor, tensor)
#This is function matmul is faster than doing it by hand

#Matrix Multiplciation Rules
#Rule 1: Inner dimensions must match, e.x. 3x2 * 2x3

#Rule 2: resulting matrix has shape of outer dimensions
# Ex 3x2 * 2x3, 3x3

#torch.mm is short version of writing torch.matmul

### One of most common errors in deep learning
#Shape Errors
#To fix shape issues, can manipulate shape by using .T, which transposes the matrix
tensor_B = torch.tensor([[1,2],
                        [3,4],
                        [5,6]])
tensor_B.T, tensor_B.shape

#Finding min, max, mean, sum
x = torch.arange(0,100,10)
x

#find min
torch.min(x), x.min()

#find max
torch.max(x), x.max()

#find mean
#torch.mean, requires floating type, not a Long
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

#Find sum
torch.sum(x), x.sum()

#Finding position of min value in tensor
x.argmin()

#finding position of max value in tensor
x.argmax()

#Reshaping, Stacking, squeezing and unsqueezing

#Reshaping - reshapes an input tensor to defined shape
#View - return view of an input tensor of certain shape but keep same memory
#Stacking - combine multiple tensors on top of each other (vstack) side by side
#Squeeze - removes all '1' dmensions from a tensor
#Unsqueeze - adds a '1' dimension to target tensor
#Permute - Return a view of the input with dimensions permuted (swapped) in a certain way

import torch

y = torch.arange(1., 10.)
y.shape, y

#add extra dimension
y_reshaped = y.reshape(1,9)
#y.reshpae(9,1) works too, y.reshape (5,2)
y_reshaped, y_reshaped.shape

#Change view
z = y.view(1,9)
z, z.shape

#Changing z changes (x), a view of a tensor seems to be working like a pointer

z[:,0] = 5
z,y

#Stack tensors
y_stacked = torch.stack([y,y,y,y], dim = 1)
y_stacked, y_stacked.shape

#Previous tensor
y_reshaped, y_reshaped.shape

#Squeezed tensor
y_reshaped.squeeze(), y_reshaped.squeeze().shape

#Unsqueezed tensor
y_squeezed = y_reshaped.squeeze()
y_unsqueezed = y_squeezed.unsqueeze(dim =0)
y_unsqueezed, y_unsqueezed.shape

#Permuted, USED A LOT FOR IMAGES
w = torch.randn(2,3,5) #[height, width, colour_channels]
w.size(), torch.permute(w, (2,0,1)).size()

#You can use ":" to select 'all' of a target dimension
TENSOR[0,0,:]

#NumPy, popular scientific python numerical computing library
#PyTorch has interaction with it

#NumPy array to tensor
import torch
import numpy as np
array = np.arange(1.0,8.0)
tensor = torch.from_numpy(array).type(torch.float32) #numpy -> pytorch makes tensor float64
array, tensor

# Reporducibility (trying to make random out of random)
#Neural network learns: random numbers -> tensor operations -> update random numbers
# -> make better representation -> keeps doing that last part

random_tensor_A = torch.rand(3,4)
random_tensor_B = torch.rand(3,4)
print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

#Make random but reproducible tensors

#Set random seed
#When creating random tensors it'll only work for one block of code
RANDOM_SEED = 42

torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3,4)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3,4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)

#Running tensors and PyTorch objects on GPUS

#1. Getting a GPU, buy my own
#2. Use google colab
#3. Use cloud computing

#!nvidia-smi, this will give you spec sheet of the gpu you connected to
#torch.cuda.is_available(), returns true or false
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

#count number of devices
torch.cuda.device_count()

#Putting tensors (and models) on GPU, goes faster

#Create a tensor (default is on CPU)
tensor = torch.tensor([1,2,3])

#Tensor not on GPU
print(tensor, tensor.device)

#Move tensor to GPU (if available)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu

#if tensor on GPU cannot become a NumPy

#To convert to NumPy convert to CPU
tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu

#Chapter 00 Exercises