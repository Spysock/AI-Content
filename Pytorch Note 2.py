# -*- coding: utf-8 -*-
"""01_pytorch_workflow_fudamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x-7AdOZlasem0zMg0OxKmU8oBT5s1iLU
"""

#Pytorch Workflow

import torch
from torch import nn
import matplotlib.pyplot as plt
import numpy as np

#1 Data Preparing and Loading

#Machine Learning is a game of two parts
#1. Get data into numerical representation
#2. Build a model to learn patterns in the numerical representation

#Lets create known data using linear regression

# Y = mx + b, weight is m, bias is b
weight = 0.7
bias = 0.3

#Create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim = 1)
y= weight * X + bias

X[:10], y[:10], len(X), len(y)

#Create a train/test split
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)

def plot_predictions(train_data = X_train,
                    train_labels = y_train,
                    test_data = X_test,
                    test_labels = y_test,
                    predictions = None):
  """
  Plots training data, test data and compares predictions
  """
  plt.figure(figsize=(10,7))

  #Plot Training data in blue
  plt.scatter(train_data,train_labels, c= 'b', s = 4, label = "Training Data")

  #Plot Training data in green
  plt.scatter(test_data,test_labels, c= 'g', s = 4, label = 'Testing Data')

  #Are there predictions?
  if predictions is not None:
    #plot predictions
    plt.scatter(test_data , predictions, c ='r', s = 4, label = "Predictions")

  #Show the legend
  plt.legend(prop={'size': 11});

plot_predictions();

#Create linear regression model class
class LinearRegressionModel(nn.Module): #Almost everything in PyTorch inherits from nn.Module
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1, #Start with random weight
                                            requires_grad = True, #can parameter be updated?
                                            dtype=torch.float,)) #pytorch loves torch.float32
    self.bias = nn.Parameter(torch.randn(1, #start with random bias
                                         requires_grad = True, #can be updated?
                                         dtype=torch.float)) #lessen bugs essentially
  #Forward method to define computation in model
  #Forward method is required for any subclass of nn.Module
  #Defines computation performed at every call
  def forward(self, x:torch.Tensor) -> torch.Tensor: #x is the input data
    return self.weights * x + self.bias #linear regression formula

### PyTorch model building essentials

"""
torch.nn - contains all of the buildings for computational graphs

torch.nn.Paramter - what parameters should our model try and learn,
often a PyTorch layer from torch.nn will set these for us

torch.nn.Module - The base class for all neural network modules,
if you subclass it you should overwrite forward()

torch.optim - this is where optimizers in pytorch live, will help with gradient descent

def forward() - All nn.Module subclasses need a forward(), (even if you overwrite it)
determines what happens with forward computation

"""
#Checking inside of model

"""
Can look at model parameters, using .parameters
"""
#Create random seed
torch.manual_seed(42)

#Create instance of model(subclass of nn.Module)
model_0 = LinearRegressionModel()

#Check parameters
list(model_0.parameters())

#List named parameters
model_0.state_dict()

#making prediction using torch.inference_mode()
"""
check models predictive power, lets see how well it predicts 'y_test' based on X_test

when we pass data through our model its gonna run it through forward method
"""
#Make predictions with model
with torch.inference_mode():
  y_preds = model_0(X_test)

y_preds

plot_predictions(predictions = y_preds)

#Train model

"""
The whole idea is to move from unknown parameters to known parameters
or poor representation to a better representation

We're gonna use a loss function to measure how poorly our models are doing

Lost function can also be called cost function or criterion in different areas

Formal Definiton: A function to measure how wrong your models predictions are to the ideal outputs, lower is better

We need to train the
  - The loss function to measure ho wrong the predictions are
  - The optimizer to take into accoun the loss of a model and adjust the models paremeters to improve loss function

  In PyTorch we need:
  - Training loop
  - Testing loop
"""
list(model_0.parameters())
model_0.state_dict()

#Setup loss function
loss_fn = nn.L1Loss()

#Set up optimizer
optimizer = torch.optim.SGD(params = model_0.parameters(),  #params is the models parameters you want to optimize
                            lr=0.01) #lr meanns learning rate, possibly most important hyperparameter, hyperparameter is a parameter the data engineer sets

"""
 Which optimizer and loss function to use?

 Comes with experience

 for linear regression, L1Loss() is good and optim.SGD() is good

 for a classification problem, use BCE (Binary cross entropy) nn.BCELoss() for the loss function
"""

#Building a training loop
"""
0. Loop through data
1. Forward pass (this involves data moving through our models 'forward' functions), also called forward propagation
2. Calculate loss (compare forward pass predictions to ground truth labels)
3.Optimize zero grad
4. Loss backward -Move backwards through the network to calculate the gradients of each parameter with respect to loss (Backpropagation)
5. Optimizer step - Uses optimizer to adjust models parameters to try and improve loss (Gradient descent)
"""

#an epoch is one loop through the data (hyperparameter)
epochs = 100

epoch_count = []
loss_values = []
test_loss_values = []

#Step zero loop through data
for epoch in range(epochs):
  #set model to training mode
  model_0.train() #train mode in PyTorch set sets all parameters that requires gradients to require gradients

  #1 Forward pass
  y_pred = model_0(X_train) #Moves forward through the NN

  #2 Calculate loss
  loss = loss_fn(y_pred, y_train)   #How wrong our model is, lower is better
  #print(loss)

  #3 Optimizer zero grad
  optimizer.zero_grad()     #Zeros the optimizer gradients

  #4 Perform backpropagation on loss with respect to parameters
  loss.backward()                                                       #Back propogation,

  #5 Step the optimizer (preform gradient descent)
  optimizer.step() #how optimizer changes will acumulate through the loop, so zero them above in step 3 for next iteration in the loop

  model_0.eval()#Turns off different settings in model not needed for evaluation/testing
  with torch.inference_mode(): #turns off gradient treacking and a
  #with torch.no_grad() you may also see torch.no_grad() which is the older way of doing torch.inference mode
    #Forward
    test_pred= model_0(X_test)

    #Calculate loss
    test_loss = loss_fn(test_pred, y_test)

  #print whats happening
  if epoch % 10 == 0:
    epoch_count.append(epoch)
    loss_values.append(loss)
    test_loss_values.append(test_loss)
    print(f"Epoch: {epoch}| Loss: {loss} | Test loss {test_loss}")
    #Print out model state_dict()
    print(model_0.state_dict())

#plot loss curve
plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label = 'Train loss')
plt.plot(epoch_count, test_loss_values, label = 'Test loss')
plt.title('training and test loss curves')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend();

with torch.inference_mode():
  y_preds_new = model_0(X_test)

model_0.state_dict()

plot_predictions(predictions=y_preds_new)

#Saving a model in pytorch

"""
Three main methods for saving and loading

-torch.save(), allows you to save pytorch object in pythons pickle format
-torch.load(), loads a saved pytorch object
-torch.nn.Module.load_state_dict(), allows you to load a models saved state dictionary

"""
#Saving our PyTorch Model
from pathlib import Path

#1. Create models directory
MODEL_PATH = Path('models')
MODEL_PATH.mkdir(parents = True, exist_ok = True)

#2. Model save path
MODEL_NAME = '01_pytorch_workflow_model_0.pth'
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

MODEL_SAVE_PATH

#3. Save the model state dict
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj= model_0.state_dict(),
           f = MODEL_SAVE_PATH)

!ls -l models

#Loading pytorch model

"""
Since model state_dict() was saved rather than the whole model, create an instance of model class and load saved state_dict()

"""
#To load a saved state_dict() we have to instantiate a new instance of our model calass
loaded_model_0 = LinearRegressionModel()

#Load saved state_dict of model_0 (this will update new instance with updated params)
loaded_model_0.load_state_dict(torch.load(f= MODEL_SAVE_PATH))

loaded_model_0.state_dict()

#Make some predictions with loaded model
loaded_model_0.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_0(X_test)

loaded_model_preds

#Putting it all together

"""
create device-agnostic code
this means if we've got a gpu our code will use it
"""
#Setup device agnostic code

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"using device: {device}")

